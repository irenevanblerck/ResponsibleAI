{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "openimages_assignment_student.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irenevanblerck/ResponsibleAI/blob/main/irenevanblerck/notebooks/openimages_assignment_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUTCUP-I6dBv"
      },
      "source": [
        "# __Responsible AI: Open Images V4__\n",
        "\n",
        "For this exercise, you are going to work with the Open Images V4 data:\n",
        "\n",
        "> Open Images is a dataset of ~9M images that have been annotated with image-level labels and object bounding boxes. The training set of V4 contains 14.6M bounding boxes for 600 object classes on 1.74M images, making it the largest existing dataset with object location annotations. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (8.4 per image on average). Moreover, the dataset is annotated with image-level labels spanning thousands of classes.\n",
        "\n",
        "We will perform an Exploratory Data Analysis (EDA) to identify (potential) instances of bias. Furthermore, you will learn to apply various fairness metrics, and debiasing techniques such as removing ('fairness through unawareness'), adding, and transforming images ('fairness through awareness') to the Open Images V4 dataset. \n",
        "\n",
        "__Documentation:__\n",
        "- [Open Images](https://storage.googleapis.com/openimages/web/index.html)\n",
        "- [OIDv4 ToolKit](https://github.com/EscVM/OIDv4_ToolKit)\n",
        "\n",
        "__Learning Objectives:__\n",
        "1. Identify, and describe (potential) instances of bias in the Open Images V4 dataset\n",
        "2. Propose, and apply appropriate fairness metrics, and debiasing techniques to the Open Images V4 dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Learning Objective 1: Identify, and describe (potential) instances of bias in the Open Images V4 dataset__"
      ],
      "metadata": {
        "id": "AA0nkf2mqaHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### __Load the meta data__\n",
        "\n",
        "Before you start with the exercises, it is good practice to create a virtual environment because it will allow you to install packages and modify your Python environment without fear of breaking packages installed in other environments. For more information on how to create such an environment, see the Anaconda tutorial [Managing Environments](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html). \n",
        "\n",
        "__Step 1:__ Install, and subsequently import the necessary Python packages, set your working directory, and load the data files containing the class descriptions and labels.\n",
        "\n",
        "Data sources (via USB or cloud storage link, ask lecturer):\n",
        "- class-descriptions\n",
        "- train-annotations-human-imagelabels\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DwH5zNUJGSqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "AJUXdLT_aTWo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __Merge the meta data__\n",
        "\n",
        "The table ```train-annotations-human-imagelabels``` contain information on the image ids, and class labels of the Open Images V4 dataset. The corresponding class descriptions can be found in the ```class-descriptions``` table. Merging the two tables will increase the interpretability of the data: As a human being, a class description (e.g. ```wedding```) is generally more informative than a class label (e.g.```'/m/081hv'```).\n",
        "   \n",
        "__Step 2:__ Merge the ```class-descriptions``` and ```train-annotations-human-imagelabels``` tables, and name it ``` train_label_description ```."
      ],
      "metadata": {
        "id": "NdAD1O94adQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "VA0xerpw7xJf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __Exploratory Data Analysis (EDA) on the metadata, Part 1__\n",
        "\n",
        "__Step 3:__ Perform an Exploratory Data Analysis (EDA) on the table ``` train_label_description ```, and visualize your findings. \n",
        "\n",
        "Questions:\n",
        "- How many classes are there?\n",
        "- What are the most common classes?  \n",
        "- Can you have multiple labels/classes in one image?\n",
        "- Which of the classes might be prone to bias (sensitive attributes)?\n",
        "\n",
        "etc.\n",
        "\n",
        "Tip: Need inspiration for your EDA, check Kaggle's [Inclusive Images Challenge](https://www.kaggle.com/c/inclusive-images-challenge/data?select=train_human_labels.csv). \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "niJJt3l8zAQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "XHPt4Nla7zn5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Step 3:__ Subset the data by selecting one value (i.e. class) in the column ```LabelName``` located within the table  ```train_label_description```. Name the table ```subset```.\n",
        "\n",
        "__Examples:__\n",
        "\n",
        "LabelName|Description\n",
        ":-----:|:-----:\n",
        "'/m/03bt1vf'| woman\n",
        "'/m/01f43'| beauty\n",
        "'/m/019nj4'| smile\n",
        "'/m/0fczf'| nurse\n",
        "'/m/0fsbk0'| health care practitioner\n",
        "'/m/027qf2'| chemical engineer\n",
        "'/m/02fbcn'| bartender\n",
        "'/m/081hv'| wedding\n",
        "\n",
        "<br>You can also use other tools to filter through the ```class-descriptions``` table. For example, I personally like to use Microsoft Excel (do not tell Nitin or Bram, they will probably laugh at me...) to find interesting classes.  \n",
        "\n",
        "<br>\n",
        "\n",
        "![Alt Text](https://media.giphy.com/media/UMyvk17PIo3SiZQWju/giphy.gif) "
      ],
      "metadata": {
        "id": "mGNyLBdQ2jIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "CECk4JN88XGi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every image in Open Images V4 can contain multiple image-level labels across hundreds of classes. For example:\n",
        "\n",
        "1. Place [wedding image 1](../../Study%20Content/Responsible%20and%20Explainable%20AI/notebooks/data/92c79c374cd829f8.jpg), [wedding image 2](../../Study%20Content/Responsible%20and%20Explainable%20AI/notebooks/data/f7cdb1d47905332a.jpg) and corresponding [meta data](../../Study%20Content/Responsible%20and%20Explainable%20AI/notebooks/data/wedding.csv) into your working directory.\n",
        "2. Run the code block below. "
      ],
      "metadata": {
        "id": "GoNgA7O65XVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import io\n",
        "%matplotlib inline\n",
        "\n",
        "example = pd.read_csv('wedding.csv')\n",
        "\n",
        "m_labels = example.Description_x.str.split().tolist()\n",
        "\n",
        "map_label_to_des = dict(zip(example.LabelName_x.values, example.Description_x.values))\n",
        "\n",
        "num_of_imgs = 2\n",
        "des_labels = []\n",
        "for i in np.arange(num_of_imgs):\n",
        "    j = [map_label_to_des.get(item, item) for item in m_labels[i]]\n",
        "    des_labels.append(j)\n",
        "    \n",
        "img_list = ['./{}.jpg'.format(id_) for id_ in example.ImageID.values]\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(25, 25)\n",
        "ax.set_axis_off()\n",
        "for n, (image, label) in enumerate(zip(img_list, des_labels)):\n",
        "    a = fig.add_subplot(num_of_imgs//1, num_of_imgs//1, n+1)\n",
        "    img = io.imread(image, 1)\n",
        "    img = np.float32(img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    plt.axis('off')\n",
        "    plt.title(label, fontsize=15)\n",
        "    plt.imshow(img)"
      ],
      "metadata": {
        "id": "-pGW0qgX8JSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "__Step 4:__ Merge the tables ```subset``` and```train_label_description``` to match the format:\n",
        "\n",
        "ImageID|Source_x|LabelName_x|Confidence_x|Description_x|LabelName_y|Confidence_y|Description_y\n",
        ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
        "000002b66c9c498e|crowdsource-verification|/m/01kcnl|1|Birthday|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/010l12|0|Roller coaster|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/012c4n|0|Cucurbita|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/012mj|1|Alcoholic beverage|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/012yh1|1|Style|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/014j1m|0|Apple|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/014l8n|0|Yogurt|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/014sv8|1|Human eye|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/01599|0|Beer|/m/081hv|0|Wedding\n",
        "\n",
        "<br>Name you newly merged table ```train_[fill in class description value]```. \n",
        "\n",
        "<br>The table should include a column filled with your chosen class label value (i.e. ```LabelName_y```), and another column containing the additional class labels (i.e. ```Description_x```) associated with a particular image (i.e. ```ImageID```). "
      ],
      "metadata": {
        "id": "2OrEfWa39P_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "xEQJ5M82ikGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you may have noticed the data contains a column ```Confidence[_x/y]```, which includes positive as well as negative label values (1 and 0 confidence respectively. \n",
        "\n",
        "> ... Positive labels are classes that have been verified to be in the image while negative labels are classes that are verified to not be in the image. Negative labels are useful because they are generally specified for classes that you may expect to appear in a scene but do not. For example, if there is a group of people in outfits on a field, you may expect there to be a ball. If there isn’t one, that would be a good negative label ([Source](https://medium.com/voxel51/loading-open-images-v6-and-custom-datasets-with-fiftyone-18b5334851c3)).\n",
        "\n",
        "A negative label can be an indication that there is bias present in the dataset. When you take a look at the wedding example, the value 'Ivory' is common in the case of a negative ```Confidence_y``` value. In other words, when the human annotator encounters an image with a ```Description_x``` value of 'Ivory' he/she is likely to incorrectly label ```Description_y``` as 'wedding' (i.e. false positive). In a traditional western wedding, the bride often dresses in white/ivory etc. But, that does not mean that all brides adhere to this clothing tradition. Take a country like Marocco, where some brides wear a Lebsa lfasiya:\n",
        "\n",
        ">Lebsa lfasiya is the traditional dress from the Fez region, also called \"ebsa lekbira\" (the great outfit). Its large size and shape and accompanying elaborate jewelry characterize this type of wedding dress. It can be white, red, or green ([Source](https://www.moroccoworldnews.com/2020/07/310720/marrying-love-and-fashion-wedding-dresses-in-morocco)).\n",
        "\n",
        "<img src=\"https://www.moroccoworldnews.com/wp-content/uploads/2020/07/Marrying-Love-Fashion-Wedding-Dresses-and-Morocco.jpg\" alt=\"drawing\" width=\"400\"/>\n",
        "\n",
        "*Figure 1. A woman wearing a traditional Maroccan wedding dress called a Lebsa lfasiya.*\n",
        "\n",
        "Quite different from a traditional western wedding dress, right?\n",
        "\n",
        "__Step 5:__ Subset the data into two datasets: one where the value of ```Confidence_y```  equals 0 named ```[fill in class description value]_negative``` , and another where the value of ```Confidence_y``` equals 1 named ```[fill in class description value]_negative```. Count the most common ```Description_x``` values for both subsets. Visualize your results. "
      ],
      "metadata": {
        "id": "XfqG2qOC_jMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "PCh4trNmEw9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Step 6:__ Did you find any instance of bias in the Open Images V4 dataset? If your answer is 'No', you need to dig deeper into the data! Elaborate on your answer."
      ],
      "metadata": {
        "id": "kf_I7jTtEzLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add your answer"
      ],
      "metadata": {
        "id": "77ggFPHNFI2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __Load the image data__\n",
        "\n",
        "Downloading the Open Images V4 data is quite cumbersome. The (meta) data files are large, and often need a good amount of preprocessing to make them ready for an ML analysis. The OIDv4 ToolKit enables you to download a subset of the data (less time spend on downloading data means more time for analysis...yeah!). Conveniently, it automatically transforms the image (meta) data into the correct format for Keras.\n",
        "\n",
        "__Step 7:__ Clone the [OIDv4 ToolKit](https://github.com/EscVM/OIDv4_ToolKit) GitHub repository. \n",
        "\n",
        "__Step 8:__ Open the cloned repository in the Command Prompt (i.e. terminal). For more information, run the code to see the tutorial below: "
      ],
      "metadata": {
        "id": "tL6v1ykuOv0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/bgSSJQolR0E\" frameborder=\"0\" allowfullscreen></iframe>')\n"
      ],
      "metadata": {
        "id": "VYEMS8lRHeYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Step 9:__ Activate your virtual environment in the Command Prompt, and follow OIDv4 ToolKit's installation procedure.  "
      ],
      "metadata": {
        "id": "pmNsSRsaIqAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Step 10:__ Follow the instructions listed under the heading '2.1 Download different classes in separated folders' of the OIDv4 ToolKit repository. Transfer the newly created image directories to your working directory. "
      ],
      "metadata": {
        "id": "OEnC8TpDMajM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __Exploratory Data Analysis (EDA) on the metadata, Part 2__\n",
        "\n",
        "Now we have loaded the image data, you want to redo some of the steps listed in part 1 of the EDA. But before we proceed with the EDA, you need to remove some rows in the ```train_[fill in class description value]``` table. If a value in ```ImageID``` cannot be linked to actual images in your directory, you need to remove the corresponding row. \n",
        "\n",
        "__Step 11:__ Create a list containing all the image names (i.e. ```ImageID```) without its extension (e.g. .jpg), and use it to filter out all the redundant rows in your ```train_[fill in class description value]``` table."
      ],
      "metadata": {
        "id": "MVyqrA0QPHrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "pCkOLN8ymdFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Step 12:__ Perform another EDA on your ```train_[fill in class description value]``` table. "
      ],
      "metadata": {
        "id": "9OagP6oLmdmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "49S5G2ivmfJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Learning Objective 2: Propose, and apply appropriate fairness metrics, and debiasing techniques to the Open Images V4 dataset (Part 1, Pre-processing)__"
      ],
      "metadata": {
        "id": "s4IAef-qqhCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __Mitigate bias__ \n",
        "\n",
        "Unfortunately, most of the current fairness enhancing techniques only apply to tabular data. Our project is focused on image data. Thus, we have to be a bit creative here. One way to mitigate bias in the pre-processing stage is by applying the 'fairness through unawareness' method, which basically means that you remove as much 'sensitive' data as possible. In our wedding example, that would encompass the rows of data where the ```Description_x``` value equals 'Ivory'. \n",
        "\n",
        "__Step 1:__ Apply the 'fairness through unawareness' method to your Open Images V4 dataset (meta/image data!). Elaborate on your approach. "
      ],
      "metadata": {
        "id": "6smHYtWdUy2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "NLLfYRd_vZtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add your answer"
      ],
      "metadata": {
        "id": "aCqkWRjrxeOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, you do not want to remove any data instances. For example, when you already have a relatively small dataset. Remember neural networks require a vast amount of training examples!  \n",
        "\n",
        "Another method to mitigate bias, which does not reduce the size of your dataset, is 'fairness through awareness'. In our wedding example, this would mean that we need to include images that depict a wider range of wedding garments. To make our dataset more inclusive (and thus representative), we could for example scrape the web for images of Lebsa lfasiya dresses, and add those to your dataset.\n",
        "\n",
        "__Step 2:__ Apply the 'fairness through awareness' method (i.e. add new instances) to your Open Images V4 dataset (meta/image data!). Elaborate on your approach.\n",
        "\n",
        "For a web scraping tutorial in Python, run the code below: "
      ],
      "metadata": {
        "id": "jEcLaNLovbRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/stIxEKR7o-c\" frameborder=\"0\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "mZ76S-a61SuT",
        "outputId": "e36af0af-8f3a-4704-d899-44f51185e397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/stIxEKR7o-c\" frameborder=\"0\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "1JnYghlF1wUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the unfortunate situation, when there is no additional data available. You can apply data augmentation techniques to your dataset to improve the performance of your ML model (i.e. reduce overfitting!):\n",
        "\n",
        "> Data Augmentation is a technique that can be used to artificially expand the size of a training set by creating modified data from the existing one ([Source](https://neptune.ai/blog/data-augmentation-in-python)).\n",
        "\n",
        "Keep in mind, the goal of these transformations is to make your dataset either more balanced and/or representative.\n",
        "\n",
        "__Step 3:__ Identify images that depict/represent marginalized groups (e.g. non-ivory/white wedding dresses, LGBTIQA+ couples) in the Open Images V4 dataset, and transfer them to a new directory. See code below:"
      ],
      "metadata": {
        "id": "3cPQXWqg11RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src_dir = #Insert image source directory\n",
        "dst_dir = #Insert image destination directory\n",
        "image_names = #Insert image name (without their extension, e.g. .jpg)\n",
        "for image_name in image_names:\n",
        "    shutil.copy(os.path.join(src_dir, image_name+'.jpg'), dst_dir)"
      ],
      "metadata": {
        "id": "9Cavx0RD6Uc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Step 4:__ Apply data augmentations techniques to your subset, and visualize your results."
      ],
      "metadata": {
        "id": "Sc-dtPKWDyyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "1YLBc-IzCrkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Step 5:__ Add the augmented images to your train set/directory."
      ],
      "metadata": {
        "id": "mJ2fAFnTRWL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Learning Objective 2: Propose, and apply appropriate fairness metrics, and debiasing techniques to the Open Images V4 dataset (Part 2, In-processing)__\n",
        "\n",
        "\n",
        "Say we want to train a binary classifier that can predict if an image depicts a wedding or not. As with the Open Images V4 dataset, our 'imaginary' dataset mostly contains images of traditional American (i.e. Western) weddings. Images depicting traditional Morrocan weddings are underrepresented in the data. See data distribution below:\n"
      ],
      "metadata": {
        "id": "9jDtOss9RR3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from textwrap import wrap\n",
        "\n",
        "classes = ['X_label: Traditional American Wedding', 'X_label: Traditional Marrocan Wedding']\n",
        "classes = [ '\\n'.join(wrap(c, 20)) for c in classes ]\n",
        "y_pos = np.arange(len(classes))\n",
        "count = [10000,1000]\n",
        "\n",
        "plt.bar(y_pos, count, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, classes)\n",
        "plt.ylabel('Number of X_labels')\n",
        "plt.title('Y_label = Wedding')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "wHmw4iW7wKKE",
        "outputId": "8f28f8f0-414f-438b-c2e3-d6b399fc37da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With correct bias initialization during training you can mitigate (some of the) bias introduced by the unbalanced dataset: By putting emphasis on the minority class the model does not spend the first few epochs just learning that the minority examples are unlikely (i.e. learning the bias). In Keras initialize the bias with the function [```bias_initializer```](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense). \n",
        "\n",
        "In the case of the log loss metric, the baseline of dumb, by-chance prediction is 0.693 for a balanced dataset. This number is obtained by predicting the prevalance or p, and value it at p = 0.5 for any class of the binary problem:\n",
        "\n",
        "Log Loss = -log(p)\n",
        "<br>p = (1 / N)\n",
        "\n",
        "N|2|3|5|7|10|15|20|30|50\n",
        ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
        "**Log Loss**|0.69|1.1|1.61|1.95|2.3|2.71|3|3.4|3.91\n",
        "\n",
        "Code examples:"
      ],
      "metadata": {
        "id": "FBfYMA2P9s_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "prev_class_1 = 0.5\n",
        "prev_class_2 = 0.5\n",
        "dumb_baseline = 0.5\n",
        "\n",
        "log_loss = - prev_class_1 * (math.log(dumb_baseline)) - prev_class_2 * (math.log(dumb_baseline))\n",
        "\n",
        "#OR\n",
        "\n",
        "#N = 2\n",
        "\n",
        "#log_loss = -(math.log(1 / N))\n",
        "\n",
        "print(log_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6EVNi-4xd05",
        "outputId": "cc36ccdc-9e89-44b2-d85f-0e5e4caf81da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6931471805599453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline of dumb, by-chance prediction value gets increasingly smaller when the data becomes more and more unbalanced. For example, a log loss value of 0.5 with p = 0.1 (i.e. prevalance of traditional Marrocan wedding labels) indicates that the model is performing poorly.\n",
        "\n",
        "p|1|2|3|5|10|20|30|40|50|60|70|90|95|97|98|99\n",
        ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
        "**Log Loss**|0.06|0.1|0.13|0.2|0.33|0.5|0.61|0.67|0.69|0.67|0.61|0.33|0.2|0.13|0.1|0.06\n",
        "\n",
        "\n",
        "A correct dumb baseline for this particular dataset would be around:"
      ],
      "metadata": {
        "id": "FV1n1wj-zTQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "prev_minority_class = 0.1\n",
        "prev_majority_class = 0.9\n",
        "\n",
        "log_loss = - prev_minority_class * (math.log(prev_minority_class)) - prev_majority_class * (math.log(prev_majority_class))\n",
        "\n",
        "print(log_loss)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzWT5XRauVOV",
        "outputId": "15053b3a-ca25-4c7e-90b1-dd783af86ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3250829733914482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you set the initial bias properly (in the last dense layer of your network), the model is likely to provide more reasonable initial guesses. Code example:"
      ],
      "metadata": {
        "id": "BDm8HW7uEzy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct_bias = np.log([prev_minority_class/prev_majority_class]) #Count of X_labels: Traditional American wedding, and traditional Marrocan wedding.\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid', bias_initializer=correct_bias))"
      ],
      "metadata": {
        "id": "OPAPg0z0PKuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, try to complement your analysis with metrics that focus on the minority classes, such as recall.\n",
        "\n",
        "For more information on bias initialization in Keras: [Classification on imbalanced data](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data). \n",
        "\n",
        "__Step 6:__ Add a custom bias_initializer to your Keras model. Retrain your model, and evaluate its output. What was the effect of the custom bias_initializer? Elaborate on your answer. "
      ],
      "metadata": {
        "id": "DGhM_gdqPTSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "oQT0aq9-Oswb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Learning Objective 2: Propose, and apply appropriate fairness metrics, and debiasing techniques to the Open Images V4 dataset (Part 3, Post-processing)__\n",
        "\n",
        "Although, the fairness libraries (e.g. aif360) currently do not support image data. You can calculate many of the metrics by hand. For instructions see, [Fairness: Evaluating for Bias](\n",
        "https://developers.google.com/machine-learning/crash-course/fairness/evaluating-for-bias).\n",
        "\n",
        "\n",
        "__Step 8:__ Apply post-processing fairness enhancing metrics to the output of your ML model (e.g. equality of opportunity). Evaluate your result. \n"
      ],
      "metadata": {
        "id": "kD7BSF6DTiRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add your code"
      ],
      "metadata": {
        "id": "9tlxBNHrQIeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __The End__\n",
        "\n",
        "![Alt Text](https://media0.giphy.com/media/27Y1W0GCKQtDq/giphy.gif) \n",
        "\n",
        "__Title - Responsible AI: Open Images V4__\n",
        "<br> __Author - Irene van Blerck__\n",
        "<br> __Created On - 11 Januari 2021__\n"
      ],
      "metadata": {
        "id": "eQRKihBF7cyN"
      }
    }
  ]
}